[
{"id":"ai_001","type":"mcq","question":"Which search algorithm uses both path cost g(n) and heuristic h(n)?","options":["A*","Uniform-cost","DFS","Hill climbing"],"answer":"A*","explanation":"A* evaluates f(n)=g(n)+h(n) to guide search efficiently."},
{"id":"ai_002","type":"short","question":"Name the activation function defined as max(0, x).","answer":"ReLU","aliases":["rectified linear unit","relu"],"explanation":"ReLU outputs 0 for negative inputs and x otherwise."},
{"id":"ai_003","type":"mcq","question":"Which is uninformed search?","options":["Greedy best-first","A*","Breadth-first search","Simulated annealing"],"answer":"Breadth-first search","explanation":"BFS uses no domain-specific heuristic."},
{"id":"ai_004","type":"mcq","question":"Which logic uses truth values beyond true/false?","options":["Propositional logic","First-order logic","Fuzzy logic","Modal logic"],"answer":"Fuzzy logic","explanation":"Fuzzy logic models degrees of truth in."},
{"id":"ai_005","type":"short","question":"What is the term for an admissible heuristic never overestimating cost?","answer":"Admissible heuristic","aliases":["admissible"],"explanation":"Admissible heuristics are optimistic upper bounds."},
{"id":"ai_006","type":"mcq","question":"Which optimization uses temperature parameter to escape local minima?","options":["Gradient descent","Simulated annealing","Tabu search","Hill climbing"],"answer":"Simulated annealing","explanation":"Annealing accepts worse moves with probability that decreases with temperature."},
{"id":"ai_007","type":"mcq","question":"Which algorithm computes optimal decision in zero-sum deterministic games with perfect information?","options":["Minimax","Q-learning","Policy gradient","Monte Carlo tree search"],"answer":"Minimax","explanation":"Minimax chooses moves to maximize worst-case utility."},
{"id":"ai_008","type":"short","question":"Name the pruning technique that removes branches provably irrelevant in minimax.","answer":"Alpha-beta pruning","aliases":["alpha beta","alpha–beta pruning","alpha beta pruning"],"explanation":"Alpha-beta tracks bounds to cut subtrees without affecting the result."},
{"id":"ai_009","type":"mcq","question":"Which is a constraint satisfaction problem (CSP) example?","options":["Sorting numbers","N-Queens","Matrix multiplication","Dijkstra"],"answer":"N-Queens","explanation":"N-Queens places queens under row/column/diagonal constraints."},
{"id":"ai_010","type":"short","question":"In Bayesian networks, what represents conditional dependencies among variables?","answer":"Directed acyclic graph","aliases":["dag"],"explanation":"A BN’s structure is a DAG encoding dependencies."},
{"id":"ai_011","type":"mcq","question":"Which sampling method generates samples using a proposal distribution and acceptance step?","options":["Gibbs sampling","Metropolis-Hastings","Rejection-free sampling","Bootstrap"],"answer":"Metropolis-Hastings","explanation":"MH proposes a move and accepts with a specific probability."},
{"id":"ai_012","type":"mcq","question":"Which is a value-based RL method?","options":["REINFORCE","Actor-Critic","Q-learning","PPO"],"answer":"Q-learning","explanation":"Q-learning learns action-value function Q(s,a)."},
{"id":"ai_013","type":"short","question":"What does the Bellman optimality equation relate recursively?","answer":"Value function","aliases":["bellman equation","optimal value function"],"explanation":"It expresses optimal V* or Q* in terms of successor values."},
{"id":"ai_014","type":"mcq","question":"Which technique balances exploration and exploitation in bandits?","options":["Greedy","UCB","Pure exploitation","Random"],"answer":"UCB","explanation":"Upper Confidence Bound adds uncertainty bonus to estimates."},
{"id":"ai_015","type":"short","question":"Name the RL algorithm that uses both policy and value networks.","answer":"Actor-Critic","aliases":["actor critic"],"explanation":"Actor updates policy; critic estimates value."},
{"id":"ai_016","type":"mcq","question":"Which neural net component introduces non-linearity?","options":["Weights","Bias","Activation","Loss"],"answer":"Activation","explanation":"Nonlinear activations allow modeling complex functions."},
{"id":"ai_017","type":"mcq","question":"Which regularization adds L2 penalty to weights?","options":["Dropout","BatchNorm","Weight decay","Early stopping"],"answer":"Weight decay","explanation":"L2 penalty discourages large parameters."},
{"id":"ai_018","type":"short","question":"Name the trick that normalizes layer inputs using batch statistics.","answer":"Batch Normalization","aliases":["batchnorm","batch normalization"],"explanation":"BN stabilizes and speeds up training."},
{"id":"ai_019","type":"mcq","question":"Which method randomly zeros activations during training?","options":["Dropout","LayerNorm","Weight decay","Gradient clipping"],"answer":"Dropout","explanation":"Dropout prevents co-adaptation by random masking."},
{"id":"ai_020","type":"short","question":"What is transferring a pretrained model to a new task called?","answer":"Transfer learning","aliases":["fine-tuning","finetuning"],"explanation":"Reuse learned representations for related tasks."},
{"id":"ai_021","type":"mcq","question":"Which optimizer uses first and second moment estimates of gradients?","options":["SGD","RMSProp","Adam","Adagrad"],"answer":"Adam","explanation":"Adam combines momentum and adaptive learning rates."},
{"id":"ai_022","type":"mcq","question":"Which CNN layer reduces spatial resolution?","options":["Convolution","Pooling","Fully connected","Activation"],"answer":"Pooling","explanation":"Pooling aggregates local neighborhoods."},
{"id":"ai_023","type":"short","question":"Name the trick for exploding gradients mitigation by bounding gradient norm.","answer":"Gradient clipping","aliases":["clip gradients","gradient norm clipping"],"explanation":"Clipping prevents huge updates."},
{"id":"ai_024","type":"mcq","question":"Which architecture relies on self-attention rather than recurrence?","options":["LSTM","GRU","Transformer","CNN-RNN"],"answer":"Transformer","explanation":"Transformers use multi-head self-attention."},
{"id":"ai_025","type":"short","question":"What mechanism weights tokens based on pairwise relevance?","answer":"Self-attention","aliases":["attention","scaled dot-product attention"],"explanation":"Attention scores relationships among tokens."},
{"id":"ai_026","type":"mcq","question":"Which loss is common for multi-class classification with softmax?","options":["MSE","MAE","Cross-entropy","Huber"],"answer":"Cross-entropy","explanation":"Cross-entropy measures divergence between predicted and true distributions."},
{"id":"ai_027","type":"mcq","question":"Which technique augments images by flips/rotations?","options":["Regularization","Data augmentation","Normalization","Pruning"],"answer":"Data augmentation","explanation":"Augmentation enlarges training data diversity."},
{"id":"ai_028","type":"short","question":"What is removing weights with small magnitude to compress models?","answer":"Pruning","aliases":["model pruning","weight pruning"],"explanation":"Pruning sparsifies networks with minimal accuracy loss."},
{"id":"ai_029","type":"mcq","question":"Which unsupervised model learns reduced latent representation with reconstruction objective?","options":["GAN","Autoencoder","Logistic regression","SVM"],"answer":"Autoencoder","explanation":"Autoencoders compress and reconstruct input."},
{"id":"ai_030","type":"short","question":"Name the adversarial framework with generator and discriminator.","answer":"GAN","aliases":["generative adversarial network","gans"],"explanation":"GANs train a generator against a discriminator."},
{"id":"ai_031","type":"mcq","question":"Which language model unit predicts next token given previous?","options":["Encoder","Decoder","Autoencoder","Classifier"],"answer":"Decoder","explanation":"Autoregressive decoders predict next tokens."},
{"id":"ai_032","type":"mcq","question":"Which is a common word embedding method pre-Transformer era?","options":["Word2Vec","ViT","BLIP","DINO"],"answer":"Word2Vec","explanation":"Word2Vec learns dense word vectors via skip-gram/CBOW."},
{"id":"ai_033","type":"short","question":"What is the process of converting text to tokens?","answer":"Tokenization","aliases":["tokenisation"],"explanation":"Tokenization splits text into units for models."},
{"id":"ai_034","type":"mcq","question":"Which evaluation metric is common for machine translation?","options":["BLEU","ROUGE-L","PSNR","SSIM"],"answer":"BLEU","explanation":"BLEU measures n-gram overlap with references."},
{"id":"ai_035","type":"short","question":"Name the approach aligning model outputs with human preferences post-training.","answer":"Reinforcement learning from human feedback","aliases":["RLHF"],"explanation":"RLHF optimizes a reward model trained from human labels."},
{"id":"ai_036","type":"mcq","question":"Which sequence model addresses vanishing gradients with gates?","options":["Vanilla RNN","LSTM","Transformer","CNN"],"answer":"LSTM","explanation":"LSTM uses input, forget, output gates."},
{"id":"ai_037","type":"mcq","question":"Which method detects overfitting during training?","options":["Lower train loss","Higher learning rate","Validation curve","More epochs"],"answer":"Validation curve","explanation":"Track validation metrics to spot divergence."},
{"id":"ai_038","type":"short","question":"What term describes difference between training and test performance due to overfitting?","answer":"Generalization gap","aliases":["gen gap"],"explanation":"Large gap indicates poor generalization."},
{"id":"ai_039","type":"mcq","question":"Which method turns classification logits into probabilities?","options":["ReLU","Softmax","Sigmoid","Tanh"],"answer":"Softmax","explanation":"Softmax normalizes logits to a distribution."},
{"id":"ai_040","type":"short","question":"Name the method that linearly rescales features to zero mean and unit variance.","answer":"Standardization","aliases":["z-score normalization","standardisation"],"explanation":"Transforms to zero mean, unit variance."},
{"id":"ai_041","type":"mcq","question":"Which evaluation uses a held-out set not seen during training or validation?","options":["Train split","Validation split","Test split","Bootstrap"],"answer":"Test split","explanation":"Test set assesses generalization."},
{"id":"ai_042","type":"mcq","question":"Which technique ensembles many weak learners sequentially?","options":["Bagging","Boosting","Stacking","Pruning"],"answer":"Boosting","explanation":"Boosting focuses on errors of prior learners."},
{"id":"ai_043","type":"short","question":"What term for confidence intervals for model performance via resampling with replacement?","answer":"Bootstrap","aliases":["bootstrapping"],"explanation":"Bootstrap estimates variability by resampling data."},
{"id":"ai_044","type":"mcq","question":"Which objective encourages low intra-class and high inter-class distances in embeddings?","options":["Triplet loss","MSE","Huber","Dice"],"answer":"Triplet loss","explanation":"Triplet loss uses anchor, positive, negative."},
{"id":"ai_045","type":"short","question":"Name the metric comparing sets often used in segmentation (similar to IoU).","answer":"Dice coefficient","aliases":["f1 dice","sorensen–dice","dice score"],"explanation":"Dice measures overlap between predicted and ground truth."},
{"id":"ai_046","type":"mcq","question":"Which method perturbs inputs slightly to fool classifiers?","options":["Dropout","Adversarial attack","Mixup","Cutout"],"answer":"Adversarial attack","explanation":"Adversarial examples exploit model sensitivity."},
{"id":"ai_047","type":"mcq","question":"Which defense averages predictions over many noisy inputs?","options":["Adversarial training","Randomized smoothing","Gradient masking","Label smoothing"],"answer":"Randomized smoothing","explanation":"Smoothing certifies robustness under noise."},
{"id":"ai_048","type":"short","question":"What is the technique of soft target distribution to prevent overconfidence in training?","answer":"Label smoothing","aliases":["label smoothing regularization"],"explanation":"Assigns (1-ε) to correct class and ε to others."},
{"id":"ai_049","type":"mcq","question":"Which approach quantizes weights/activations to reduce model size?","options":["Pruning","Distillation","Quantization","Ensembling"],"answer":"Quantization","explanation":"Use lower-precision (e.g., int8) representations."},
{"id":"ai_050","type":"short","question":"Name the technique to compress a large teacher into a smaller student.","answer":"Knowledge distillation","aliases":["model distillation","distillation"],"explanation":"Student learns from teacher’s soft targets."},
{"id":"ai_051","type":"mcq","question":"Which is NOT a typical heuristic property?","options":["Admissible","Consistent","Overestimating","Monotone"],"answer":"Overestimating","explanation":"Heuristics should not overestimate if admissible."},
{"id":"ai_052","type":"mcq","question":"Which pathfinding uses heuristic only (no g-cost)?","options":["A*","Greedy best-first","Uniform-cost","IDA*"],"answer":"Greedy best-first","explanation":"Greedy selects node with lowest h(n)."},
{"id":"ai_053","type":"short","question":"What constraint-solving strategy assigns variables then backtracks on failure?","answer":"Backtracking search","aliases":["backtracking"],"explanation":"Systematically explores assignments with backtracking."},
{"id":"ai_054","type":"mcq","question":"Which inference algorithm passes messages on graphs until convergence?","options":["Variable elimination","Belief propagation","Enumeration","MCTS"],"answer":"Belief propagation","explanation":"BP passes local messages to compute marginals."},
{"id":"ai_055","type":"short","question":"In HMMs, which algorithm finds most likely hidden state sequence?","answer":"Viterbi","aliases":["viterbi algorithm"],"explanation":"Viterbi uses dynamic programming for MAP path."},
{"id":"ai_056","type":"mcq","question":"Which loss is common for binary classification with logits?","options":["MSE","Binary cross-entropy","KL divergence","L1"],"answer":"Binary cross-entropy","explanation":"BCE suits two-class problems."},
{"id":"ai_057","type":"mcq","question":"Which method prevents data leakage in time series?","options":["Random shuffle","K-fold","Walk-forward validation","LOOCV"],"answer":"Walk-forward validation","explanation":"Respects temporal order."},
{"id":"ai_058","type":"short","question":"Name the technique of mixing pairs of samples and labels linearly.","answer":"Mixup","aliases":["mix-up"],"explanation":"Regularizes by convex combinations of inputs/targets."},
{"id":"ai_059","type":"mcq","question":"Which attention variant normalizes across features instead of batch/sequence?","options":["BatchNorm","LayerNorm","InstanceNorm","GroupNorm"],"answer":"LayerNorm","explanation":"LayerNorm normalizes across hidden features."},
{"id":"ai_060","type":"short","question":"What is the positional encoding function family often used in Transformers?","answer":"Sinusoidal positional encoding","aliases":["sinusoidal encoding","positional encoding"],"explanation":"Uses sine/cosine with different frequencies."},
{"id":"ai_061","type":"mcq","question":"Which gradient-based method accumulates velocity of past gradients?","options":["Nesterov momentum","AdamW","Adagrad","Rprop"],"answer":"Nesterov momentum","explanation":"Uses lookahead gradient for faster convergence."},
{"id":"ai_062","type":"mcq","question":"Which architecture is best suited for image classification with global receptive fields via attention?","options":["RNN","Transformer (ViT)","Naive Bayes","SVM"],"answer":"Transformer (ViT)","explanation":"ViT applies self-attention to image patches."},
{"id":"ai_063","type":"short","question":"What term describes mapping high-dimensional inputs to lower-dimensional latent codes in VAEs?","answer":"Encoder","aliases":["inference network"],"explanation":"VAE encoder outputs mean and variance."},
{"id":"ai_064","type":"mcq","question":"Which divergence is used in VAE objective?","options":["JS divergence","KL divergence","Wasserstein","Total variation"],"answer":"KL divergence","explanation":"VAE minimizes KL between approximate posterior and prior."},
{"id":"ai_065","type":"short","question":"Name the algorithm combining tree search with random rollouts in games.","answer":"Monte Carlo tree search","aliases":["mcts"],"explanation":"MCTS balances exploration and exploitation via UCT."},
{"id":"ai_066","type":"mcq","question":"Which method updates Q-values using bootstrapped targets from next state?","options":["REINFORCE","SARSA","Policy gradient","Evolution strategies"],"answer":"SARSA","explanation":"On-policy TD uses (s,a,r,s',a')."},
{"id":"ai_067","type":"mcq","question":"Which is a policy gradient improvement technique using clipped objective?","options":["DQN","TRPO","PPO","Q-learning"],"answer":"PPO","explanation":"PPO stabilizes updates via clipping ratio."},
{"id":"ai_068","type":"short","question":"What’s the technique of decaying ε in ε-greedy exploration over time?","answer":"Epsilon decay","aliases":["epsilon annealing"],"explanation":"Reduces exploration as learning progresses."},
{"id":"ai_069","type":"mcq","question":"Which method reuses past transitions via a buffer in deep RL?","options":["Experience replay","Eligibility traces","Imitation learning","Curriculum"],"answer":"Experience replay","explanation":"Replay breaks correlation and improves sample efficiency."},
{"id":"ai_070","type":"short","question":"What is training on demonstrations before RL fine-tuning called?","answer":"Imitation learning","aliases":["behavior cloning"],"explanation":"Learns policy from expert trajectories."},
{"id":"ai_071","type":"mcq","question":"Which NLP task labels words with grammatical categories?","options":["Parsing","POS tagging","NER","Summarization"],"answer":"POS tagging","explanation":"Part-of-speech tagging assigns word categories."},
{"id":"ai_072","type":"mcq","question":"Which evaluation is typical for summarization?","options":["BLEU","ROUGE","FID","Inception score"],"answer":"ROUGE","explanation":"ROUGE measures overlap with reference summaries."},
{"id":"ai_073","type":"short","question":"What’s the process of converting characters to integer IDs before embedding?","answer":"Indexing","aliases":["vocabulary indexing","id mapping"],"explanation":"Maps tokens to indices via a vocabulary."},
{"id":"ai_074","type":"mcq","question":"Which computer vision task outputs class and bounding box?","options":["Classification","Detection","Segmentation","Depth estimation"],"answer":"Detection","explanation":"Object detection localizes and classifies objects."},
{"id":"ai_075","type":"short","question":"Name the metric measuring overlap of predicted and true bounding boxes.","answer":"Intersection over Union","aliases":["IoU","jaccard"],"explanation":"IoU = area of intersection / union."},
{"id":"ai_076","type":"mcq","question":"Which segmentation type labels every pixel with a class?","options":["Instance segmentation","Semantic segmentation","Detection","Tracking"],"answer":"Semantic segmentation","explanation":"Semantic segmentation assigns a class per pixel."},
{"id":"ai_077","type":"mcq","question":"Which trick stabilizes GAN training by changing distance measure?","options":["Label smoothing","WGAN with Wasserstein loss","Dropout","BN"],"answer":"WGAN with Wasserstein loss","explanation":"Wasserstein distance provides smoother gradients."},
{"id":"ai_078","type":"short","question":"What’s the term for catastrophic drop in performance when a model is fine-tuned on new data only?","answer":"Catastrophic forgetting","aliases":["forgetting"],"explanation":"Forgetting occurs without rehearsal or regularization."},
{"id":"ai_079","type":"mcq","question":"Which technique prevents forgetting by penalizing changes to important weights?","options":["CutMix","EWC","Early stopping","Bagging"],"answer":"EWC","explanation":"Elastic Weight Consolidation uses Fisher information."},
{"id":"ai_080","type":"short","question":"What is training-time technique to stop when validation loss stops improving?","answer":"Early stopping","aliases":["validation early stopping"],"explanation":"Halts training to avoid overfitting."},
{"id":"ai_081","type":"mcq","question":"Which loss is typical for image reconstruction with robustness to outliers?","options":["L1","L2","Cross-entropy","Focal"],"answer":"L1","explanation":"L1 (MAE) is less sensitive to outliers than L2."},
{"id":"ai_082","type":"mcq","question":"Which scheduling lowers learning rate when a plateau is detected?","options":["Cosine annealing","Step decay","ReduceLROnPlateau","Cyclical"],"answer":"ReduceLROnPlateau","explanation":"Monitors metric and reduces LR on stagnation."},
{"id":"ai_083","type":"short","question":"Name the method that cycles LR between bounds to find good regions.","answer":"Cyclical learning rate","aliases":["clr"],"explanation":"LR varies periodically to improve training."},
{"id":"ai_084","type":"mcq","question":"Which technique aggregates predictions of multiple differently seeded models?","options":["Dropout","Ensembling","Quantization","Distillation"],"answer":"Ensembling","explanation":"Averaging models improves robustness and accuracy."},
{"id":"ai_085","type":"short","question":"What’s the name for mapping logits to calibrated probabilities after training using temperature?","answer":"Temperature scaling","aliases":["logit calibration"],"explanation":"Adjusts softmax temperature to calibrate outputs."},
{"id":"ai_086","type":"mcq","question":"Which evaluation checks performance across shifts like noise or corruptions?","options":["IID test","Robustness testing","Train loss","Data augmentation"],"answer":"Robustness testing","explanation":"Tests under distribution shift conditions."},
{"id":"ai_087","type":"mcq","question":"Which tokenization splits words into frequent subword units?","options":["Whitespace","Wordpiece/BPE","Character","N-gram"],"answer":"Wordpiece/BPE","explanation":"Subword tokenizers handle rare words efficiently."},
{"id":"ai_088","type":"short","question":"What’s the mapping from discrete tokens to continuous vectors?","answer":"Embedding","aliases":["word embedding","token embedding"],"explanation":"Embeddings place tokens in vector space."},
{"id":"ai_089","type":"mcq","question":"Which evaluation uses precision and recall averaged across classes?","options":["Accuracy","Macro F1","AUC","R^2"],"answer":"Macro F1","explanation":"Macro F1 averages F1 per class equally."},
{"id":"ai_090","type":"short","question":"What technique penalizes confidence on incorrect classes in dense detection?","answer":"Focal loss","aliases":["focal"],"explanation":"Focal loss down-weights easy negatives."},
{"id":"ai_091","type":"mcq","question":"Which approach trains an agent to mimic human actions via supervised learning?","options":["Q-learning","Imitation learning","SARSA","PPO"],"answer":"Imitation learning","explanation":"Learns policy from demonstrations."},
{"id":"ai_092","type":"mcq","question":"Which module computes attention across channels in vision models?","options":["SE block","BatchNorm","Dropout","MaxPool"],"answer":"SE block","explanation":"Squeeze-and-Excitation reweights channels adaptively."},
{"id":"ai_093","type":"short","question":"Name the property of heuristic h where h(n) ≤ c(n,n')+h(n') for edges.","answer":"Consistency","aliases":["monotonic heuristic"],"explanation":"Consistent heuristics ensure non-decreasing f-values."},
{"id":"ai_094","type":"mcq","question":"Which rule-based system component applies production rules to a working memory?","options":["Inference engine","Perceptron","Decoder","Optimizer"],"answer":"Inference engine","explanation":"Inference engine matches rules and executes actions."},
{"id":"ai_095","type":"short","question":"What’s the measure of alignment between two vectors commonly used in similarity search?","answer":"Cosine similarity","aliases":["cosine"],"explanation":"Cosine measures angle-based similarity."},
{"id":"ai_096","type":"mcq","question":"Which training trick randomly erases rectangular regions of images?","options":["Cutout","Mixup","Dropout","Color jitter"],"answer":"Cutout","explanation":"Cutout improves robustness via occlusion."},
{"id":"ai_097","type":"mcq","question":"Which is a graph neural network operation?","options":["Message passing","Softmax","Pooling only","Fourier transform"],"answer":"Message passing","explanation":"GNNs aggregate neighbor messages."},
{"id":"ai_098","type":"short","question":"What is the typical non-linearity used in GNNs after aggregation?","answer":"ReLU","aliases":["relu"],"explanation":"ReLU is commonly used after graph convolutions."},
{"id":"ai_099","type":"mcq","question":"Which technique aligns multimodal image-text representations?","options":["Word2Vec","CLIP-style contrastive learning","Autoencoder","LDA"],"answer":"CLIP-style contrastive learning","explanation":"Contrastive loss brings matching pairs together."},
{"id":"ai_100","type":"short","question":"Name the process of adapting a large pretrained model to a specific downstream task with few parameters updated.","answer":"Parameter-efficient fine-tuning","aliases":["PEFT","adapters","lora"],"explanation":"PEFT methods like LoRA update low-rank adapters only."}
]