[
{"id":"dl_001","type":"mcq","question":"Which activation helps mitigate vanishing gradients better than sigmoid?","options":["Sigmoid","Tanh","ReLU","Softmax"],"answer":"ReLU","explanation":"ReLU keeps positive gradients."},
{"id":"dl_002","type":"short","question":"Name the technique that randomly drops units during training.","answer":"Dropout","aliases":["drop out"],"explanation":"Regularizes by preventing co-adaptation."},
{"id":"dl_003","type":"mcq","question":"Which network uses gates to control information flow?","options":["MLP","LSTM","CNN","Autoencoder"],"answer":"LSTM","explanation":"Input/forget/output gates."},
{"id":"dl_004","type":"mcq","question":"Which layer reduces spatial size in CNNs?","options":["Conv","Pooling","BatchNorm","Dense"],"answer":"Pooling","explanation":"Aggregates local neighborhoods."},
{"id":"dl_005","type":"short","question":"What mechanism attends to relevant tokens by pairwise scoring?","answer":"Self-attention","aliases":["attention"],"explanation":"Weights tokens based on similarity."},
{"id":"dl_006","type":"mcq","question":"Which architecture replaces recurrence with attention?","options":["GRU","Transformer","RNN","LSTM"],"answer":"Transformer","explanation":"Parallel self-attention."},
{"id":"dl_007","type":"mcq","question":"Which optimizer uses first and second moment estimates?","options":["SGD","RMSProp","Adam","Adagrad"],"answer":"Adam","explanation":"Adaptive learning rates plus momentum."},
{"id":"dl_008","type":"short","question":"Name the normalization across features within a layer.","answer":"Layer Normalization","aliases":["layernorm","layer normalization"],"explanation":"Independent of batch size."},
{"id":"dl_009","type":"mcq","question":"Which loss is standard for multi-class classification?","options":["MSE","MAE","Cross-entropy","Huber"],"answer":"Cross-entropy","explanation":"Matches softmax probabilities to labels."},
{"id":"dl_010","type":"short","question":"What encoding injects order info with sin/cos at different frequencies?","answer":"Sinusoidal positional encoding","aliases":["positional encoding"],"explanation":"Used in Transformers."},
{"id":"dl_011","type":"mcq","question":"Which CNN trick expands receptive field without reducing resolution?","options":["Stride","Pooling","Dilated convolution","1x1 conv"],"answer":"Dilated convolution","explanation":"Inserts holes in kernel."},
{"id":"dl_012","type":"mcq","question":"Which block reweights channels using squeeze and excitation?","options":["ResBlock","SE block","Inverted residual","Bottleneck"],"answer":"SE block","explanation":"Channel attention."},
{"id":"dl_013","type":"short","question":"What training trick rescales logits to calibrate probabilities?","answer":"Temperature scaling","aliases":["logit temperature"],"explanation":"Post-hoc calibration."},
{"id":"dl_014","type":"mcq","question":"Which RNN variant simplifies LSTM with fewer gates?","options":["GRU","BiRNN","ConvLSTM","IndRNN"],"answer":"GRU","explanation":"Update and reset gates."},
{"id":"dl_015","type":"short","question":"What is freezing layers and training a few top layers called?","answer":"Fine-tuning","aliases":["transfer learning"],"explanation":"Transfers pretrained features."},
{"id":"dl_016","type":"mcq","question":"Which regularizer penalizes confidence on incorrect classes?","options":["Label smoothing","Dropout","Weight decay","Early stopping"],"answer":"Label smoothing","explanation":"Prevents overconfident predictions."},
{"id":"dl_017","type":"mcq","question":"Which GAN variant uses Wasserstein distance for stability?","options":["DCGAN","WGAN","LSGAN","CycleGAN"],"answer":"WGAN","explanation":"Provides smoother gradients."},
{"id":"dl_018","type":"short","question":"Name the bottleneck model that reconstructs input from compressed code.","answer":"Autoencoder","aliases":["ae"],"explanation":"Learns latent representations."},
{"id":"dl_019","type":"mcq","question":"Which technique mixes two images and labels linearly?","options":["Cutout","Mixup","CutMix","Color jitter"],"answer":"Mixup","explanation":"Regularization via linear mixing."},
{"id":"dl_020","type":"short","question":"What method bounds gradient norm to handle exploding gradients?","answer":"Gradient clipping","aliases":["clip gradients"],"explanation":"Prevents unstable updates."},
{"id":"dl_021","type":"mcq","question":"Which schedule decays LR following cosine without restarts?","options":["Step","Exponential","Cosine annealing","ReduceLROnPlateau"],"answer":"Cosine annealing","explanation":"Smoothly lowers LR."},
{"id":"dl_022","type":"mcq","question":"Which architecture classifies image patches with attention?","options":["ResNet","EfficientNet","ViT","DenseNet"],"answer":"ViT","explanation":"Vision Transformer."},
{"id":"dl_023","type":"short","question":"What is updating only small low-rank adapters in fine-tuning called?","answer":"LoRA","aliases":["low-rank adaptation","peft"],"explanation":"Parameter-efficient tuning."},
{"id":"dl_024","type":"mcq","question":"Which loss is used for object detection bounding box regression?","options":["Cross-entropy","Smooth L1 (Huber)","Dice","Triplet"],"answer":"Smooth L1 (Huber)","explanation":"Balances robustness and precision."},
{"id":"dl_025","type":"short","question":"Name the measure of overlap for segmentation similar to Jaccard.","answer":"Dice coefficient","aliases":["dice score"],"explanation":"2|A∩B|/(|A|+|B|)."},
{"id":"dl_026","type":"mcq","question":"Which module stabilizes training by normalizing layer inputs with batch stats?","options":["LayerNorm","BatchNorm","InstanceNorm","GroupNorm"],"answer":"BatchNorm","explanation":"Uses batch mean/variance."},
{"id":"dl_027","type":"mcq","question":"Which decoder-only LM predicts next token autoregressively?","options":["BERT","T5","GPT-style","Autoencoder"],"answer":"GPT-style","explanation":"Autoregressive decoding."},
{"id":"dl_028","type":"short","question":"What is training with human preference reward models called?","answer":"RLHF","aliases":["reinforcement learning from human feedback"],"explanation":"Aligns generations to preferences."},
{"id":"dl_029","type":"mcq","question":"Which convolution uses 1x1 to change channel dimension?","options":["Depthwise","Pointwise","Dilated","Transposed"],"answer":"Pointwise","explanation":"1x1 conv mixes channels."},
{"id":"dl_030","type":"short","question":"What is convolution that upsamples feature maps?","answer":"Transposed convolution","aliases":["deconvolution"],"explanation":"Learned upsampling in decoders."},
{"id":"dl_031","type":"mcq","question":"Which trick drops random rectangular regions of an image?","options":["Dropout","Cutout","Mixup","Label smoothing"],"answer":"Cutout","explanation":"Improves robustness."},
{"id":"dl_032","type":"mcq","question":"Which attention variant computes cross-modal alignment for image–text?","options":["Self-attention","Cross-attention","Sparse attention","Local attention"],"answer":"Cross-attention","explanation":"Queries attend to other modality keys/values."},
{"id":"dl_033","type":"short","question":"Name the loss that brings matched pairs closer and pushes mismatched apart in embedding space.","answer":"Contrastive loss","aliases":["infoNCE","nt-xent"],"explanation":"Basis of CLIP/SimCLR."},
{"id":"dl_034","type":"mcq","question":"Which trick randomly swaps image patches and labels proportionally?","options":["Mixup","Cutout","CutMix","Jitter"],"answer":"CutMix","explanation":"Paste patch from another image."},
{"id":"dl_035","type":"short","question":"What is fine-tuning with frozen backbone and new head called?","answer":"Linear probing","aliases":["linear evaluation"],"explanation":"Train a linear classifier on frozen features."},
{"id":"dl_036","type":"mcq","question":"Which module adds residual connections enabling very deep nets?","options":["SE block","Residual block","Inverted residual","Hourglass"],"answer":"Residual block","explanation":"Skip connections ease optimization."},
{"id":"dl_037","type":"mcq","question":"Which head predicts both class and box in one stage?","options":["Faster R-CNN","YOLO/RetinaNet","R-CNN","RCNN+SVR"],"answer":"YOLO/RetinaNet","explanation":"One-stage detectors."},
{"id":"dl_038","type":"short","question":"Name the diffusion model process that denoises step-by-step to generate samples.","answer":"Reverse diffusion","aliases":["denoising diffusion"],"explanation":"Iterative denoising from noise."},
{"id":"dl_039","type":"mcq","question":"Which training technique augments by random color/geometry operations?","options":["Data augmentation","Pruning","Quantization","Distillation"],"answer":"Data augmentation","explanation":"Improves generalization."},
{"id":"dl_040","type":"short","question":"What is compressing models by removing small-magnitude weights called?","answer":"Pruning","aliases":["weight pruning"],"explanation":"Reduces size and latency."},
{"id":"dl_041","type":"mcq","question":"Which quantization reduces weights/activations precision to int8?","options":["Pruning","Quantization","Distillation","Low rank"],"answer":"Quantization","explanation":"Post-training or QAT."},
{"id":"dl_042","type":"mcq","question":"Which student–teacher training uses soft targets to compress?","options":["Ensembling","Distillation","Bagging","Pruning"],"answer":"Distillation","explanation":"Student mimics teacher."},
{"id":"dl_043","type":"short","question":"Name the technique of normalizing channels in groups.","answer":"Group Normalization","aliases":["groupnorm"],"explanation":"Less batch-size sensitive than BN."},
{"id":"dl_044","type":"mcq","question":"Which objective arranges (anchor, positive, negative) to learn embeddings?","options":["Triplet loss","Dice","IoU","Huber"],"answer":"Triplet loss","explanation":"Enforces margin between pairs."},
{"id":"dl_045","type":"short","question":"What is training on noisy labels with loss that downweights easy negatives in dense detection?","answer":"Focal loss","aliases":["focal"],"explanation":"Addresses class imbalance."},
{"id":"dl_046","type":"mcq","question":"Which module aggregates multi-scale context via parallel dilations?","options":["ASPP","FPN","SPP","BiFPN"],"answer":"ASPP","explanation":"Atrous Spatial Pyramid Pooling."},
{"id":"dl_047","type":"mcq","question":"Which architecture upsamples using skip connections from encoder to decoder?","options":["U-Net","ResNet","DenseNet","Inception"],"answer":"U-Net","explanation":"Combines low-level and high-level features."},
{"id":"dl_048","type":"short","question":"What is aligning outputs to human preferences during finetuning called?","answer":"RLHF","aliases":["reinforcement learning from human feedback"],"explanation":"Reward model guides updates."},
{"id":"dl_049","type":"mcq","question":"Which training trick mixes batches to stabilize statistics and memory?","options":["Gradient accumulation","Ghost batch norm","Label smoothing","CutMix"],"answer":"Gradient accumulation","explanation":"Emulates larger batch sizes."},
{"id":"dl_050","type":"short","question":"Name the method of freezing BatchNorm stats during fine-tuning for stability.","answer":"BN freezing","aliases":["freeze batchnorm"],"explanation":"Fix running mean/variance."},
{"id":"dl_051","type":"mcq","question":"Which encoder–decoder text model uses cross-attention from decoder to encoder outputs?","options":["GPT","BERT","T5","ViT"],"answer":"T5","explanation":"Seq2seq Transformer."},
{"id":"dl_052","type":"mcq","question":"Which loss is common for segmentation pixelwise classification?","options":["Cross-entropy","Triplet","Center loss","CTC"],"answer":"Cross-entropy","explanation":"Per-pixel classification."},
{"id":"dl_053","type":"short","question":"What is the log-sum-exp trick used to improve?","answer":"Numerical stability","aliases":["stability of softmax"],"explanation":"Avoids overflow/underflow."},
{"id":"dl_054","type":"mcq","question":"Which RNN objective aligns unsegmented input and labels (ASR)?","options":["CTC","Cross-entropy","Triplet","Dice"],"answer":"CTC","explanation":"Connectionist Temporal Classification."},
{"id":"dl_055","type":"short","question":"What is the technique of reusing cached key/value states in decoder inference?","answer":"KV caching","aliases":["key value cache"],"explanation":"Speeds autoregressive decoding."},
{"id":"dl_056","type":"mcq","question":"Which positional approach makes attention limited to fixed windows?","options":["Absolute","ALiBi","Local attention","Relative"],"answer":"Local attention","explanation":"Reduces complexity."},
{"id":"dl_057","type":"mcq","question":"Which module fuses features across scales in detectors?","options":["FPN","SPP","Squeeze","Ghost"],"answer":"FPN","explanation":"Feature Pyramid Network."},
{"id":"dl_058","type":"short","question":"What is generating adversarial examples to test robustness called?","answer":"Adversarial attack","aliases":["fgsm","pgd attack"],"explanation":"Small perturbations fool models."},
{"id":"dl_059","type":"mcq","question":"Which defense provides certified robustness via Gaussian noise?","options":["Adversarial training","Randomized smoothing","Dropout","Mixup"],"answer":"Randomized smoothing","explanation":"Averages noisy predictions."},
{"id":"dl_060","type":"short","question":"What do we call using adapters or LoRA instead of full fine-tuning?","answer":"PEFT","aliases":["parameter-efficient fine-tuning"],"explanation":"Fewer trainable params."},
{"id":"dl_061","type":"mcq","question":"Which attention scales dot products by 1/sqrt(dk)?","options":["Additive","Scaled dot-product","Cosine","Multiplicative only"],"answer":"Scaled dot-product","explanation":"Stabilizes gradients."},
{"id":"dl_062","type":"mcq","question":"Which technique improves training by smoothing label distribution?","options":["Label smoothing","Dropout","Cutout","Pruning"],"answer":"Label smoothing","explanation":"Reduces overconfidence."},
{"id":"dl_063","type":"short","question":"Name the regularizer that adds λ||W||^2 to loss.","answer":"Weight decay","aliases":["l2 regularization"],"explanation":"Penalizes large weights."},
{"id":"dl_064","type":"mcq","question":"Which vision model uses depthwise separable conv to reduce FLOPs?","options":["VGG","MobileNet","ResNet","DenseNet"],"answer":"MobileNet","explanation":"Factorizes conv into depthwise + pointwise."},
{"id":"dl_065","type":"short","question":"What is training stopping when validation loss stops improving called?","answer":"Early stopping","aliases":["stop early"],"explanation":"Prevents overfitting."},
{"id":"dl_066","type":"mcq","question":"Which loss is used for metric learning to cluster classes tightly?","options":["Center loss","Dice","IoU","CTC"],"answer":"Center loss","explanation":"Pulls features toward class centers."},
{"id":"dl_067","type":"mcq","question":"Which method augments text by randomly deleting, swapping, inserting?","options":["EDA","Back-translation","SpecAugment","Mixup"],"answer":"EDA","explanation":"Easy Data Augmentation for NLP."},
{"id":"dl_068","type":"short","question":"Name the speech augmentation method masking time/frequency bands.","answer":"SpecAugment","aliases":["spectrogram augmentation"],"explanation":"Improves ASR robustness."},
{"id":"dl_069","type":"mcq","question":"Which training uses teacher forcing to speed sequence learning?","options":["Reinforcement","Self-supervised","Supervised","Unsupervised"],"answer":"Supervised","explanation":"Use ground-truth tokens during training."},
{"id":"dl_070","type":"short","question":"What is caching and reusing attention KV states across layers called in multi-query?","answer":"Multi-query attention","aliases":["MQA"],"explanation":"Shared keys/values per head group."},
{"id":"dl_071","type":"mcq","question":"Which architecture predicts segmentation masks for each instance?","options":["Semantic segmentation","Instance segmentation","Detection","Classification"],"answer":"Instance segmentation","explanation":"Mask R-CNN style outputs."},
{"id":"dl_072","type":"mcq","question":"Which tokenizer splits into frequent subwords?","options":["Whitespace","Character","BPE/WordPiece","SentencePiece only"],"answer":"BPE/WordPiece","explanation":"Efficient handling of rare words."},
{"id":"dl_073","type":"short","question":"What is aligning embeddings of matching image-text pairs called?","answer":"Contrastive learning","aliases":["clip-style training"],"explanation":"Brings positives closer in latent space."},
{"id":"dl_074","type":"mcq","question":"Which trick averages checkpoints across steps to smooth weights?","options":["SWALP","EMA/Polyak averaging","Warmup","Gradient clipping"],"answer":"EMA/Polyak averaging","explanation":"Stabilizes performance."},
{"id":"dl_075","type":"short","question":"What is warming LR from small to target over initial steps called?","answer":"Learning rate warmup","aliases":["lr warmup"],"explanation":"Helps stabilize early training."},
{"id":"dl_076","type":"mcq","question":"Which decoder technique prevents repetition in text generation?","options":["Greedy","Beam only","Nucleus/top-p sampling","Deterministic"],"answer":"Nucleus/top-p sampling","explanation":"Samples from top probability mass."},
{"id":"dl_077","type":"mcq","question":"Which component enables very deep nets by reusing features densely?","options":["ResNet","DenseNet","MobileNet","Inception"],"answer":"DenseNet","explanation":"Dense connections."},
{"id":"dl_078","type":"short","question":"Name the parameter that controls number of attention heads.","answer":"Number of heads","aliases":["num heads","heads"],"explanation":"Multi-head attention parallel heads."},
{"id":"dl_079","type":"mcq","question":"Which loss is used to align speech frames and labels in ASR?","options":["CTC","Cross-entropy","Dice","Triplet"],"answer":"CTC","explanation":"Handles unsegmented alignment."},
{"id":"dl_080","type":"short","question":"What is reusing frozen features for small downstream datasets called?","answer":"Transfer learning","aliases":["pretraining and fine-tuning"],"explanation":"Leverages prior knowledge."},
{"id":"dl_081","type":"mcq","question":"Which data mixing concatenates inputs along batch dimension to simulate big batch?","options":["Gradient accumulation","Batch concat","Ghost BN","Mixup"],"answer":"Gradient accumulation","explanation":"Accumulates grads before update."},
{"id":"dl_082","type":"mcq","question":"Which normalization works well for small batches in vision?","options":["BatchNorm","GroupNorm","LayerNorm","InstanceNorm"],"answer":"GroupNorm","explanation":"Independent of batch size."},
{"id":"dl_083","type":"short","question":"Name the term for reducing model precision to speed inference.","answer":"Quantization","aliases":["int8 quant"],"explanation":"Reduces memory and compute."},
{"id":"dl_084","type":"mcq","question":"Which loss encourages margin between classes in embeddings?","options":["Triplet","Center","ArcFace","IoU"],"answer":"ArcFace","explanation":"Additive angular margin loss."},
{"id":"dl_085","type":"short","question":"What is combining model snapshots across epochs called?","answer":"Stochastic Weight Averaging","aliases":["SWA"],"explanation":"Improves generalization."},
{"id":"dl_086","type":"mcq","question":"Which text model masks tokens and predicts them bidirectionally?","options":["GPT","BERT","T5","Seq2seq RNN"],"answer":"BERT","explanation":"Masked language modeling."},
{"id":"dl_087","type":"mcq","question":"Which method prunes connections during training and regrows them?","options":["SNIP","Lottery Ticket","RigL","Magnitude-only"],"answer":"RigL","explanation":"Dynamic sparse training."},
{"id":"dl_088","type":"short","question":"What do we call aligning model logits with human labels using a learned reward?","answer":"Reward modeling","aliases":["preference model"],"explanation":"Used in RLHF pipelines."},
{"id":"dl_089","type":"mcq","question":"Which schedule cycles LR between bounds during training?","options":["Step","Cosine","Cyclical learning rate","ReduceLROnPlateau"],"answer":"Cyclical learning rate","explanation":"LR oscillates to find good regions."},
{"id":"dl_090","type":"short","question":"What is the receptive field growth trick using 1x1 + 3x3 stacks?","answer":"Inception modules","aliases":["inception"],"explanation":"Multi-branch convolutions."},
{"id":"dl_091","type":"mcq","question":"Which module fuses multi-scale features with learnable top-down edges?","options":["FPN","BiFPN","PANet","ASPP"],"answer":"BiFPN","explanation":"Weighted bidirectional pyramid."},
{"id":"dl_092","type":"mcq","question":"Which normalization is typical in Transformers?","options":["BatchNorm","LayerNorm","GroupNorm","InstanceNorm"],"answer":"LayerNorm","explanation":"Across features per token."},
{"id":"dl_093","type":"short","question":"Name the trick adding noise to activations/weights to regularize.","answer":"Noisy layers","aliases":["stochastic regularization"],"explanation":"Improves robustness."},
{"id":"dl_094","type":"mcq","question":"Which diffusion sampler speeds generation with fewer steps?","options":["DDPM","DDIM","Euler","Ancestral only"],"answer":"DDIM","explanation":"Deterministic sampling variant."},
{"id":"dl_095","type":"short","question":"What is the term for stability technique scaling residual branches?","answer":"Residual scaling","aliases":["rescale residuals"],"explanation":"Stabilizes deep residual nets."},
{"id":"dl_096","type":"mcq","question":"Which data augmentation mixes images in frequency domain?","options":["Mixup","FMix","CutMix","Jitter"],"answer":"FMix","explanation":"Mask in Fourier domain."},
{"id":"dl_097","type":"mcq","question":"Which inference method decodes text by considering several hypotheses at once?","options":["Greedy","Beam search","Top-k","Nucleus"],"answer":"Beam search","explanation":"Keeps best beams each step."},
{"id":"dl_098","type":"short","question":"Name the operation that turns feature maps to a single value per channel.","answer":"Global average pooling","aliases":["gap"],"explanation":"Reduces to vector before dense."},
{"id":"dl_099","type":"mcq","question":"Which schedule reduces LR when monitored metric plateaus?","options":["Cosine","ReduceLROnPlateau","Step","Cyclical"],"answer":"ReduceLROnPlateau","explanation":"Adaptive LR scheduling."},
{"id":"dl_100","type":"short","question":"What is constraining weights to low rank via factorization called?","answer":"Low-rank factorization","aliases":["rank factorization"],"explanation":"Compresses layers and speeds inference."}
]